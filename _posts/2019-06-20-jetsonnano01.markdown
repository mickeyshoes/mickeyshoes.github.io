---
layout: post
title:  "Jetson Nano를 이용한 물체인식 코드 돌려보기"
date:   2019-06-20 23:15:00
last_modified_at:  2019-06-20 23:15:00
excerpt: "구조부터 한번 찬찬히 살펴보자. 주로 사용했던 부분위주로 설명하겠다. 1번은 SD카드에 운영체제를 설치하여 넣어주면 된다. 4번은 랜 포트 5번은 USB 포트이다...."
categories: lab
tags:  Jetson Nano
image:
  feature: mcikey-OpryHouse.jpg
  topPosition: 0px
bgContrast: dark
bgGradientOpacity: darker
syntaxHighlighter: no
---


Set up
--


#### 구조
***
<div class="img img--fullContainer img--14xLeading" style="background-image: url({{ site.baseurl_posts_img }}jetson-nano-dev-kit-top-r6-HR.png);"></div>

사진 출처 - https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit#intro

구조부터 한번 찬찬히 살펴보자.

주로 사용했던 부분위주로 설명하겠다.

1번은 SD카드에 운영체제를 설치하여 넣어주면 된다.

4번은 랜 포트

5번은 USB 포트이다. 작성자는 키보드와 마우스, 무선랜카드를 장착해서 사용했다.

6번에는 모니터를 연결하여 사용

8번은 jetson nano 충전기를 연결

9번은 라즈베리파이 카메라를 연결하는 포트이다.

구조에 대한 설명이 끝났다. 이제 본격적인 setting에 들어가보자

#### Setting
***
일단 운영체제를 먼저 설치해야 한다.

ubuntu를 사용해서 Jetson Nano를 실행해보자.

일단 SD카드를 포맷부터 해야한다.

다운로드는 하단 링크 참조.

[윈도우](https://www.sdcard.org/downloads/formatter/eula_windows/)

[MAC, LINUX](https://www.balena.io/etcher/)

그 다음 공식홈페이지에서 Jetson Nano에 쓰일 SD Card에 image 파일을 받아와야 한다.

직접 nvidia 공식홈페이지에서 다운로드를 받아도 되고 하단의 링크에서 다운로드 받아도 된다.

용량이 꽤나 큰 편이니 주의하자.

[Jetson Nano Developer Kit SD Card Image](https://developer.nvidia.com/embedded/dlc/jetson-nano-dev-kit-sd-card-image)

다운로드가 완료 되었다면 SD카드에 파일을 넣고 Jetson의 2번 포트에 넣어주자

Jetson을 다루기 위해 모니터, 마우스, 무선랜포트 or 랜선, 전원을 연결하고 실행시킨다.

실행시키면 ubuntu 설치가 이루어 질 것이다.

운영체제 설치가 완료되면 이제 Jetson을 동작시켜보자 !

참고한 동영상 : [친절한 언박싱](https://www.youtube.com/watch?v=km0yT99eVTY)


Jetson Nano 동작
--
***

자 이제 Jetson에 라즈베리파이 카메라를 연결해주고 카메라를 이용한 물체인식 코드를 돌려 볼 차례이다.

터미널을 켜서
<blockquote class="u--startsWithDoubleQuote">sudo apt-get install git cmake</blockquote>
를 실행한다.

깃허브에 있는 소스를 clone 해온다.
<blockquote class="u--startsWithDoubleQuote">git clone https://github.com/dusty-nv/jetson-inference</blockquote>


clone이 완료가 되었다면 clone한 폴더로 이동하자
<blockquote class="u--startsWithDoubleQuote">cd jetson-inference</blockquote>

그 다음 submodule update를 해준다.
<blockquote class="u--startsWithDoubleQuote">git submodule update --init</blockquote>

build 라는 폴더를 하나 만들어주고, build로 이동
<blockquote class="u--startsWithDoubleQuote">mkdir build  
cd build</blockquote>

build 에서,
<blockquote class="u--startsWithDoubleQuote">cmake ../  
make  
sudo make install</blockquote> 를 입력하여 설치해주자

이제 실행할 준비가 되었다!

터미널에서
<blockquote class="u--startsWithDoubleQuote">cd /jetson-inference/build/aarch64/bin</blockquote> 로 이동해주고

imagenet-camera googlenet 실행
<blockquote class="u--startsWithDoubleQuote">./imagenet-camera googlenet</blockquote>

실행하고 나서 심한 렉이 걸리거나, 화면이 멈춘다면 jetson의 전원을 한번 뽑아서 재부팅 해주면 된다.

imagenet-camera를 실행하였다면, 라즈베리파이 카메라로 잡히는 물체를 jetson이 어떤 사물인지 판단하여 알려준다.

필자는 jetson에게 가발판정을 받았다..T-T...
여러분은 그렇지 않길..

그럼 jetson이 인식한 내용을 보자

<div class="img img--fullContainer img--14xLeading" style="background-image: url({{ site.baseurl_posts_img }}JetsonNanoBananaCheck.png);"></div>
-바나나 모양 필통을 바나나로 인식했다. 가격이 의심되지만 이정도면 애교라고 생각한다.

<div class="img img--fullContainer img--14xLeading" style="background-image: url({{ site.baseurl_posts_img }}JetsonNanoLighterCheck.png);"></div>
-라이터를 잘 집어냈다. 역시 비싼 기계는 다르다.

<div class="img img--fullContainer img--14xLeading" style="background-image: url({{ site.baseurl_posts_img }}JetsonNanoMonitorCheck.png);"></div>
-모니터도 잘 간파했다.

이렇게 사물을 잘 판단하는 Jetson. 4차산업혁명의 리더라고 봐도 무방하겠다.

그렇다면 과연 자기 자신은 잘 판단할까?

<div class="img img--fullContainer img--14xLeading" style="background-image: url({{ site.baseurl_posts_img }}JetsonNanoSelfCheck.png);"></div>
..  

....  

자신을 그저 테이프 플레이어 정도로 밖에 생각하지 못하는 아쉬운 리더였다.

참고한 동영상 : [Jetson Nano :Vision Recognition Neural Network Demo](https://www.youtube.com/watch?v=k5pXXmTkPNM)

#### ++bonus
***
기왕 Jetson에 라즈베리파이 카메라를 연결한 김에 얼굴 인식도 같이 해보자.

github에 존재하는 CSI-camera를 이용할 것이다.

터미널을 열고 일단 clone 해오자.
<blockquote class="u--startsWithDoubleQuote">git clone https://github.com/JetsonHacksNano/CSI-camera</blockquote>

clone을 다 해왔으면 clone한 폴더로 이동
<blockquote class="u--startsWithDoubleQuote">cd CSI-camera</blockquote>

그렇다면 이제 카메라가 잘 동작 하는지 테스트를 해보자.
<blockquote class="u--startsWithDoubleQuote">gst-launch-1.0 nvarguscamerasrc ! 'video/x-raw(memory:NVMM),width=3820, height=2464, framerate=21/1, format=NV12' ! nvvidconv flip-method=0 ! 'video/x-raw,width=960, height=616' ! nvvidconv ! nvegltransform ! nveglglessink -e</blockquote>

위 명령어를 입력하면 카메라가 비추는 방향을 모니터에서 확인이 가능하다.

혹시나 이 카메라로 인터넷 방송을 시작하겠다면 말리진 않겠다.  
아마 금방 카메라를 내리치거나 방송을 그만 둘 지도 모른다.

그럼 이제 얼굴 인식을 해보자.

얼굴 인식을 하는 module은 python으로 만들어졌다. python이 안쓰이는 곳은 과연 어디일까 궁금할 정도다.

한번 실행해보자.
<blockquote class="u--startsWithDoubleQuote">python face_detect.py</blockquote>

실행을 하게 되면 초록색 네모는 사람의 이목구비를, 파란색 네모는 사람의 전체적인 얼굴형을 잡는 듯 하다.

<div class="img img--fullContainer img--14xLeading" style="background-image: url({{ site.baseurl_posts_img }}JetsonNanoHumanCheck.png);"></div>
구글에서 가져온 이미지로 한번 실행해보았다. 아주 실행이 잘 되는 듯 하다.

그렇다면 이번에는 Jetson이 가발 판정을 내리지 않을 것이라고 판단하고 작성자의 얼굴을 찍어보았으나 미동도 없다.

내일부터 Jetson의 집은 분리수거장이다.

참고한 동영상 : [Jetson Nano + Rasberry Pi Camera](https://www.youtube.com/watch?v=dHvb225Pw1s)
